{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/vrushtikhajanchi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Importing the essential libraries\n",
    "import requests\n",
    "import nltk\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "import csv\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For news collected from Google finance:\n",
    "\n",
    "df1 = pd.read_excel('1.xlsx')\n",
    "#delete time stamp in the current_time column\n",
    "df1['current_date'] = df1['Current_Time'].str.slice(0,10)\n",
    "#generagte multiplier value\n",
    "df1['value'] = df1[df1[\"Publish_Date\"].str.contains(\"ago\")].Publish_Date.str.slice(0,1)\n",
    "df1['value'] = df1['value'].replace(np.nan, 0)\n",
    "#Converting this column to datetime format\n",
    "df1['current_date'] = pd.to_datetime(df1['current_date'])\n",
    "# convert \"value\" from String to int\n",
    "df1 = df1.astype({'value':'int'})\n",
    "df1['publish_date_formatted'] = df1['current_date'] \\\n",
    "                                - df1['days'] * df1['value'] * np.timedelta64(1, 'D') \\\n",
    "                                - df1['weeks'] * df1['value'] * np.timedelta64(1, 'W') \\\n",
    "                                - df1['months'] * df1['value'] * np.timedelta64(1, 'M')\n",
    "def mdy_to_ymd(d):\n",
    "    return datetime.strptime(d, '%b %d, %Y').strftime('%Y-%m-%d')\n",
    "mask = df1['value'] == 0\n",
    "df1.loc[mask, 'publish_date_formatted'] = df1.loc[mask, 'Publish_Date'].apply(mdy_to_ymd)\n",
    "df1['publish_date_formatted'] = pd.to_datetime(df1['publish_date_formatted'])\n",
    "#delete time stamp in the publish_date_formatted column\n",
    "df1['publish_date_formatted'] = pd.to_datetime(df1['publish_date_formatted']).dt.date\n",
    "df1.to_excel('MasterFile6.0.xlsx',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For news collected from Microsoft Bing:\n",
    "\n",
    "df1 = pd.read_excel('Databricks_bing.xlsx')\n",
    "df1['Publish_Date'] = df1['Publish_Date'].replace(np.nan, \"-\")\n",
    "#generating new indicator column for date that's in \"x d\"/\"y mon\"\n",
    "\n",
    "df1['days'] = df1['Publish_Date'].str.contains('d')\n",
    "df1['days'] = df1['days'].map({True: 1, False: 0})\n",
    "\n",
    "df1['months'] = df1['Publish_Date'].str.contains('mon')\n",
    "df1['months'] = df1['months'].map({True: 1, False: 0})\n",
    "\n",
    "#generagte multiplier value\n",
    "df1['d value'] = df1[df1[\"Publish_Date\"].str.contains(\"d\")].Publish_Date.str.slice(0,-1)\n",
    "\n",
    "df1['mon value'] = df1[df1[\"Publish_Date\"].str.contains(\"mon\")].Publish_Date.str.slice(0,-3)\n",
    "\n",
    "df1['d value'] = df1['d value'].replace(np.nan, 0)\n",
    "df1['mon value'] = df1['mon value'].replace(np.nan, 0)\n",
    "\n",
    "#delete time stamp in the current_time column\n",
    "df1['current_date'] = df1['Current_Time'].str.slice(0,10)\n",
    "\n",
    "#Converting this column to datetime format\n",
    "df1['current_date'] = pd.to_datetime(df1['current_date'])\n",
    "# convert \"d value\" & \"mon value\" from String to int\n",
    "df1 = df1.astype({'d value':'int'})\n",
    "df1 = df1.astype({'mon value':'int'})\n",
    "\n",
    "# Manipulating weird weirdo format\n",
    "df1['publish_date_formatted'] = df1['current_date'] \\\n",
    "                                - df1['days'] * df1['d value'] * np.timedelta64(1, 'D') \\\n",
    "                                - df1['months'] * df1['mon value'] * np.timedelta64(1, 'M')\n",
    "df1['publish_date_formatted'] = pd.to_datetime(df1['publish_date_formatted'])\n",
    "#delete time stamp in the publish_date_formatted column\n",
    "df1['publish_date_formatted'] = pd.to_datetime(df1['publish_date_formatted']).dt.date\n",
    "\n",
    "# Keep publish_date_formatted column NaN for those news that don't have a publish date\n",
    "\n",
    "df1.loc[df1['Publish_Date'] == \"-\", ['publish_date_formatted']] = \"-\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing irrelevant news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('MasterFile6.0.xlsx', sheet_name=\"Asana\")\n",
    "df = df[df[\"Title\"].str.contains(\"Asanas\")==False]\n",
    "df = df[df[\"Title\"].str.contains(\"asanas\")==False]\n",
    "df = df[df[\"Title\"].str.contains(\"Yoga\")==False]\n",
    "df = df[df[\"Title\"].str.contains(\"yoga\")==False]\n",
    "df.to_excel('asana.xlsx',index=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Masterfile = pd.ExcelFile('Copy_of_MasterFile7.0.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_name = ['Databricks', 'Confluent', 'Asana', 'Airtable', 'Reltio', 'Netskope', 'SkyHigh_Security',\\\n",
    "                'Domino_Data_Lab', 'Mapbox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(Masterfile, 'Databricks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating functions for each step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data cleaning\n",
    "def preprocess(df):\n",
    "    df.drop(columns=['Publish_Date','Current_Time', 'days', 'weeks', 'months', 'current_date', 'value', 'Picture'],\\\n",
    "                   inplace=True)\n",
    "    df.columns = ['Headline', 'URL', 'Publisher', 'Summary', 'Date', 'Company_Name']\n",
    "    #print(df.isna().sum())\n",
    "    df.dropna(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Sentence tokenization\n",
    "def get_headlines(df):\n",
    "    sentences=[]\n",
    "    for h in df.Headline:\n",
    "        if type(h) != str:\n",
    "            h=str(h)\n",
    "        sentences.append(h)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment Analysis\n",
    "def sentiment_analysis(sent):\n",
    "    textblob_sentiment=[]\n",
    "    for s in sent:\n",
    "        txt= TextBlob(s)\n",
    "        a= txt.sentiment.polarity\n",
    "        b= txt.sentiment.subjectivity\n",
    "        textblob_sentiment.append([s,a,b])\n",
    "        df_textblob = pd.DataFrame(textblob_sentiment, columns =['Sentence', 'Polarity', 'Subjectivity'])\n",
    "    return df_textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment categorization\n",
    "def sentiment_type(df):\n",
    "    print('News with positive sentiment:', len(df[df.Polarity>0]))\n",
    "    print('News with negative sentiment:', len(df[df.Polarity<0]))\n",
    "    print('News with neutral sentiment:', len(df[df.Polarity==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def after_process(df):\n",
    "    df['Sentiment'] = df['Polarity'].apply(lambda x: 'Negative' if (x<0) \\\n",
    "                            else 'Positive' if (x>0) \\\n",
    "                            else 'Neutral')\n",
    "    df['Sentiment Score'] = df['Polarity'] * 100\n",
    "    df['Sentiment Score']  = round(df['Sentiment Score'], 2)\n",
    "    df = pd.get_dummies(df, columns=['Sentiment'])\n",
    "    df.rename(columns={'Sentiment_Negative':'Negative', 'Sentiment_Positive':'Positive',\\\n",
    "                            'Sentiment_Neutral':'Neutral'}, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Percentile threshold method\n",
    "def percentile_scores(df, pos_perc, neg_perc):\n",
    "#     df_pos = df[df['Sentiment Score'] >0]\n",
    "#     df_neg = df[df['Sentiment Score'] <0]\n",
    "    \n",
    "    pos_score = df[df['Sentiment Score']>0]['Sentiment Score']\n",
    "    neg_score = df[df['Sentiment Score']<0]['Sentiment Score']\n",
    "    \n",
    "    pos_thresh = np.percentile(pos_score, pos_perc)\n",
    "    neg_thresh = np.percentile(neg_score, neg_perc)\n",
    "\n",
    "    print(str(pos_perc)+ \" percentile for positive sentiment score:\", pos_thresh)\n",
    "    print(str(neg_perc)+ \" percentile for negative sentiment score:\", neg_thresh)\n",
    "    \n",
    "    df = df[(df['Sentiment Score']<=neg_thresh) | (df['Sentiment Score']>=pos_thresh)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Word_Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_impact_words(df):\n",
    "    df_high_impact_words = pd.DataFrame(columns = df.columns)\n",
    "\n",
    "    # Stores the high impact words in a list\n",
    "    techwords_list = []\n",
    "\n",
    "    # Read high impact words from CSV file\n",
    "    with open('AWS_tech_list.csv', 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            word = row[0].lower()\n",
    "            techwords_list.append(word)\n",
    "\n",
    "    # The tech word list might have dupliacte words. This can cause the word cloud\n",
    "    # to falsely grow. To resolve this, we need to remove the duplicate words from\n",
    "    # the word cloud.\n",
    "    techwords_list = [*set(techwords_list)]\n",
    "    \n",
    "    # Stores the list of words which are hit\n",
    "    techwords_hit = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        #print(row['Date'], row['Headline'])\n",
    "        row['Word_Hit'] = \"\"\n",
    "        df_high_impact_words = df_high_impact_words.append(row, \n",
    "                                                   ignore_index = True)\n",
    "        for hit_word in techwords_list:\n",
    "            # Use below if we need more hits, like debut and debuts. But that\n",
    "            # also matches words like AI in HAIL.\n",
    "            # if hit_word in row['Headline'].lower():\n",
    "            if hit_word in row['Headline'].lower().split():\n",
    "                techwords_hit.append(hit_word)\n",
    "                row['Word_Hit'] = hit_word\n",
    "                df_high_impact_words = df_high_impact_words.append(row, \n",
    "                                                   ignore_index = True)\n",
    "                # print(hit_word + \" --> \" + row['Headline'])\n",
    "   \n",
    "    print(\"Number of headlines : \" + str(df_high_impact_words['Headline'].count()))\n",
    "    # print(techwords_hit)\n",
    "    print(\"Tech words hit : \" + str(len(techwords_hit)))\n",
    "\n",
    "    return df_high_impact_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Databricks\n",
      "75 percentile for positive sentiment score: 40.0\n",
      "25 percentile for negative sentiment score: -23.75\n",
      "Number of headlines : 52\n",
      "Tech words hit : 24\n",
      "Confluent\n",
      "75 percentile for positive sentiment score: 37.5\n",
      "25 percentile for negative sentiment score: -29.3775\n",
      "Number of headlines : 58\n",
      "Tech words hit : 24\n",
      "Asana\n",
      "75 percentile for positive sentiment score: 50.0\n",
      "25 percentile for negative sentiment score: -27.5\n",
      "Number of headlines : 73\n",
      "Tech words hit : 20\n",
      "Airtable\n",
      "75 percentile for positive sentiment score: 50.0\n",
      "25 percentile for negative sentiment score: -22.5\n",
      "Number of headlines : 42\n",
      "Tech words hit : 8\n",
      "Reltio\n",
      "75 percentile for positive sentiment score: 40.0\n",
      "25 percentile for negative sentiment score: -17.78\n",
      "Number of headlines : 15\n",
      "Tech words hit : 4\n",
      "Netskope\n",
      "75 percentile for positive sentiment score: 50.0\n",
      "25 percentile for negative sentiment score: -17.5\n",
      "Number of headlines : 35\n",
      "Tech words hit : 13\n",
      "SkyHigh_Security\n",
      "75 percentile for positive sentiment score: 80.0\n",
      "25 percentile for negative sentiment score: -20.0\n",
      "Number of headlines : 50\n",
      "Tech words hit : 26\n",
      "Domino_Data_Lab\n",
      "75 percentile for positive sentiment score: 50.0\n",
      "25 percentile for negative sentiment score: -42.5\n",
      "Number of headlines : 22\n",
      "Tech words hit : 5\n",
      "Mapbox\n",
      "75 percentile for positive sentiment score: 49.83\n",
      "25 percentile for negative sentiment score: -29.09\n",
      "Number of headlines : 22\n",
      "Tech words hit : 2\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary to store DataFrames for each company\n",
    "# company_dfs = {}\n",
    "tech_word_dfs = {}\n",
    "\n",
    "company_name\n",
    "# iterate over each company and read its data into a DataFramefor x in company_name:\n",
    "for x in company_name:\n",
    "    print(x)\n",
    "    #print(\"\\n\")\n",
    "    df = pd.read_excel(Masterfile, x)\n",
    "    company_list = [x] * len(df)  # create list of company names with same length as dataframe\n",
    "    df['Company_Name'] = company_list  # add new column with company name\n",
    "    \n",
    "    df = preprocess(df)\n",
    "    \n",
    "    sentences = get_headlines(df)\n",
    "    \n",
    "    sentiment_df = sentiment_analysis(sentences)\n",
    "\n",
    "    sentiment_df2 = after_process(sentiment_df)\n",
    "    \n",
    "    #new code to remove Nans\n",
    "    df.reset_index(inplace=True)\n",
    "    df.drop(columns='index', inplace=True)\n",
    "    sentiment_df2['Date']= df['Date']\n",
    "    sentiment_df2['URL']= df.URL\n",
    "    sentiment_df2['Headline'] = df.Headline\n",
    "    sentiment_df2['Company_Name'] = df['Company_Name'] # add company name to the new DataFrame\n",
    "\n",
    "    sentiment_df2['Sentiment_type']=sentiment_df2.Polarity.apply(lambda x: 'Positive' if x>0 else 'Negative' if x<0 \\\n",
    "                                                       else 'Neutral')     \n",
    "    sentiment_df2.rename(columns={'Sentence':'Sentiment headline'}, inplace=True)\n",
    "    \n",
    "    sentiment_df2=percentile_scores(sentiment_df2, 75, 25)\n",
    "    \n",
    "    # Create high impact word hit list\n",
    "    tech_word_df = high_impact_words(sentiment_df2)\n",
    "    tech_word_dfs[x] = tech_word_df\n",
    "        \n",
    "# write DataFrames for each company to separate sheets in the same Excel file\n",
    "with pd.ExcelWriter('Company_TechWordCloud_2_OG.xlsx') as writer:\n",
    "    for x, df in tech_word_dfs.items():\n",
    "        df.to_excel(writer, sheet_name=x, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
